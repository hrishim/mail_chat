docker login nvcr.io --username '$oauthtoken' --password "${NGC_API_KEY}"

export LOCAL_NIM_CACHE=~/.cache/nim
mkdir -p "$LOCAL_NIM_CACHE"
chmod 777 "$LOCAL_NIM_CACHE"

docker run -d --name meta-llama3-8b-instruct --gpus all -e NGC_API_KEY -v "$LOCAL_NIM_CACHE:/opt/nim/.cache" -u $(id -u) -p 8000:8000 nvcr.io/nim/meta/llama3-8b-instruct:1.0.0

curl -X 'POST' \
   "http://0.0.0.0:8000/v1/completions" \
   -H "accept: application/json" \
   -H "Content-Type: application/json" \
   -d '{"model": "meta/llama3-8b-instruct", "prompt": "Once upon a time", "max_tokens": 64}'


Embedding Model: NV-Embed-QA, 

Reranking Models: 
- nv-rerankqa-mistral-4b-v3: https://build.nvidia.com/nvidia/nv-rerankqa-mistral-4b-v3/
- llama-3.2-nv-rerankqa-1b-v2: https://build.nvidia.com/nvidia/llama-3_2-nv-rerankqa-1b-v2


LLM: 
- meta-llama3-8b-instruct:  https://build.nvidia.com/meta/llama3-8b
docker run -it --rm \
 --name llama3-8b-instruct \ 
 --gpus all -e NGC_API_KEY=$NGC_API_KEY \
 -v /home/hrishi/.cache/nim:/opt/nim/.cache \
 -u 1000 -p 8000:8000 --shm-size=2g \
 --ulimit memlock=-1 --ipc=host \
 nvcr.io/nim/meta/llama3-8b-instruct:1.0.0


- llama-3_1-8b-instruct: https://build.nvidia.com/meta/llama-3_1-8b-instruct [Model Fails to run]
docker run -it --rm \
    --gpus all \
    --shm-size=16GB \
    -e NGC_API_KEY=$NGC_API_KEY\
    -v /home/hrishi/.cache/nim:/opt/nim/.cache \
    -u $(id -u) \
    -p 8000:8000 \
    nvcr.io/nim/meta/llama-3.1-8b-instruct:latest


- deepseek-r1-distill-llama-8b: [Model fails to run]
   - Model: https://build.nvidia.com/deepseek-ai/deepseek-r1-distill-llama-8b, 
   - NIM: https://catalog.ngc.nvidia.com/orgs/nim/teams/deepseek-ai/containers/deepseek-r1-distill-llama-8b
docker run -it --rm \
    --gpus all \
    --shm-size=16GB \
    -e NGC_API_KEY=$NGC_API_KEY \
    -v /home/hrishi/.cache/nim:/opt/nim/.cache \
    -u $(id -u) \
    -p 8000:8000 \
    nvcr.io/nim/deepseek-ai/deepseek-r1-distill-llama-8b:1.5.2

- mistral-nemo-12b-instruct: https://build.nvidia.com/nv-mistralai/mistral-nemo-12b-instruct [Model Fails to run]
docker run -it --rm \
    --gpus all \
    --shm-size=16GB \
    -e NGC_API_KEY=$NGC_API_KEY \
    -v /home/hrishi/.cache/nim:/opt/nim/.cache \
    -u $(id -u) \
    -p 8000:8000 \
    nvcr.io/nim/nv-mistralai/mistral-nemo-12b-instruct:latest

- mixtral-8x7b-instruct: https://build.nvidia.com/mistralai/mixtral-8x7b-instruct/modelcard [Model Fails to run]
docker run -it --rm \
    --gpus all \
    --shm-size=16GB \
    -e NGC_API_KEY=$NGC_API_KEY \
    -v /home/hrishi/.cache/nim:/opt/nim/.cache \
    -u $(id -u) \
    -p 8000:8000 \
    nvcr.io/nim/mistralai/mixtral-8x7b-instruct-v01:latest

ERROR 2025-03-23 15:20:21.681 utils.py:21] Could not find a profile that is currently runnable with the detected hardware.

Other Models compatible with meta-llama3-8b-instruct
Multilingual-E5-Large: A general-purpose embedding model also used in RAG workflows 
Llama-Text-Embed-V2: Specifically designed to align with the LLaMA family of models, making it a natural fit for meta-llama3
Cohere Embed Models: Such as embed-english-v3 or embed-multilingual-v3, which provide high-quality embeddings for multilingual or English text

When selecting an embedding model:
- Ensure it supports your specific use case (e.g., query and passage embeddings).
- Check its compatibility with your deployment infrastructure (e.g., NVIDIA NIMs, Hugging Face, etc.).
- Test its alignment with meta-llama3-8b-instruct to ensure semantic coherence in retrieved results. 

Evaluating RAG flows: https://medium.com/@zilliz_learn/optimizing-rag-applications-a-guide-to-methodologies-metrics-and-evaluation-tools-for-enhanced-a9ae3d9c7149

python data_prep.py --vectordb-dir hrishim_vectordb --save-frequency 10 hrishi_gmail.mbox

python data_prep.py --vectordb-dir hrishim_vectordb --save-frequency 10 --resume hrishi_gmail.mbox