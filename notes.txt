docker login nvcr.io --username '$oauthtoken' --password "${NGC_API_KEY}"

export LOCAL_NIM_CACHE=~/.cache/nim
mkdir -p "$LOCAL_NIM_CACHE"
chmod 777 "$LOCAL_NIM_CACHE"

docker run -d --name meta-llama3-8b-instruct --gpus all -e NGC_API_KEY -v "$LOCAL_NIM_CACHE:/opt/nim/.cache" -u $(id -u) -p 8000:8000 nvcr.io/nim/meta/llama3-8b-instruct:1.0.0
docker run --rm -it --name meta-llama3-8b-instruct --gpus all -e NGC_API_KEY -v "$LOCAL_NIM_CACHE:/opt/nim/.cache" -u $(id -u) -p 8000:8000 nvcr.io/nim/meta/llama3-8b-instruct:1.0.0

curl -X 'POST' \
   "http://0.0.0.0:8000/v1/completions" \
   -H "accept: application/json" \
   -H "Content-Type: application/json" \
   -d '{"model": "meta/llama3-8b-instruct", "prompt": "Once upon a time", "max_tokens": 64}'


WARNING 03-27 06:09:08.948 logging.py:314] Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
INFO 03-27 06:09:08.957 api_server.py:456] Serving endpoints:
  0.0.0.0:8000/openapi.json
  0.0.0.0:8000/docs
  0.0.0.0:8000/docs/oauth2-redirect
  0.0.0.0:8000/metrics
  0.0.0.0:8000/v1/health/ready
  0.0.0.0:8000/v1/health/live
  0.0.0.0:8000/v1/models
  0.0.0.0:8000/v1/version
  0.0.0.0:8000/v1/chat/completions
  0.0.0.0:8000/v1/completions


Embedding Model: NV-Embed-QA, 

Reranking Models: 
- nv-rerankqa-mistral-4b-v3: https://build.nvidia.com/nvidia/nv-rerankqa-mistral-4b-v3/
- llama-3.2-nv-rerankqa-1b-v2: https://build.nvidia.com/nvidia/llama-3_2-nv-rerankqa-1b-v2


LLM: 
- meta-llama3-8b-instruct:  https://build.nvidia.com/meta/llama3-8b

docker run -it --rm \
 --name llama3-8b-instruct \ 
 --gpus all -e NGC_API_KEY=$NGC_API_KEY \
 -v /home/hrishi/.cache/nim:/opt/nim/.cache \
 -u 1000 -p 8000:8000 --shm-size=2g \
 --ulimit memlock=-1 --ipc=host \
 nvcr.io/nim/meta/llama3-8b-instruct:1.0.0


- llama-3_1-8b-instruct: https://build.nvidia.com/meta/llama-3_1-8b-instruct [Model Fails to run]
docker run -it --rm \
    --gpus all \
    --shm-size=16GB \
    -e NGC_API_KEY=$NGC_API_KEY\
    -v /home/hrishi/.cache/nim:/opt/nim/.cache \
    -u $(id -u) \
    -p 8000:8000 \
    nvcr.io/nim/meta/llama-3.1-8b-instruct:latest


- deepseek-r1-distill-llama-8b: [Model fails to run]
   - Model: https://build.nvidia.com/deepseek-ai/deepseek-r1-distill-llama-8b, 
   - NIM: https://catalog.ngc.nvidia.com/orgs/nim/teams/deepseek-ai/containers/deepseek-r1-distill-llama-8b
docker run -it --rm \
    --gpus all \
    --shm-size=16GB \
    -e NGC_API_KEY=$NGC_API_KEY \
    -v /home/hrishi/.cache/nim:/opt/nim/.cache \
    -u $(id -u) \
    -p 8000:8000 \
    nvcr.io/nim/deepseek-ai/deepseek-r1-distill-llama-8b:1.5.2

- mistral-nemo-12b-instruct: https://build.nvidia.com/nv-mistralai/mistral-nemo-12b-instruct [Model Fails to run]
docker run -it --rm \
    --gpus all \
    --shm-size=16GB \
    -e NGC_API_KEY=$NGC_API_KEY \
    -v /home/hrishi/.cache/nim:/opt/nim/.cache \
    -u $(id -u) \
    -p 8000:8000 \
    nvcr.io/nim/nv-mistralai/mistral-nemo-12b-instruct:latest

- mixtral-8x7b-instruct: https://build.nvidia.com/mistralai/mixtral-8x7b-instruct/modelcard [Model Fails to run]
docker run -it --rm \
    --gpus all \
    --shm-size=16GB \
    -e NGC_API_KEY=$NGC_API_KEY \
    -v /home/hrishi/.cache/nim:/opt/nim/.cache \
    -u $(id -u) \
    -p 8000:8000 \
    nvcr.io/nim/mistralai/mixtral-8x7b-instruct-v01:latest

ERROR 2025-03-23 15:20:21.681 utils.py:21] Could not find a profile that is currently runnable with the detected hardware.

Other Models compatible with meta-llama3-8b-instruct
Multilingual-E5-Large: A general-purpose embedding model also used in RAG workflows 
Llama-Text-Embed-V2: Specifically designed to align with the LLaMA family of models, making it a natural fit for meta-llama3
Cohere Embed Models: Such as embed-english-v3 or embed-multilingual-v3, which provide high-quality embeddings for multilingual or English text

When selecting an embedding model:
- Ensure it supports your specific use case (e.g., query and passage embeddings).
- Check its compatibility with your deployment infrastructure (e.g., NVIDIA NIMs, Hugging Face, etc.).
- Test its alignment with meta-llama3-8b-instruct to ensure semantic coherence in retrieved results. 

Evaluating RAG flows: https://medium.com/@zilliz_learn/optimizing-rag-applications-a-guide-to-methodologies-metrics-and-evaluation-tools-for-enhanced-a9ae3d9c7149

python data_prep.py --vectordb-dir hrishim_vectordb --save-frequency 10 hrishi_gmail.mbox

python data_prep.py --vectordb-dir hrishim_vectordb --save-frequency 10 --resume hrishi_gmail.mbox

-----------------------

Links
https://python.langchain.com/docs/tutorials/rag/ 

https://blog.epsilla.com/advanced-rag-optimization-boosting-answer-quality-on-complex-questions-through-query-decomposition-e9d836eaf0d5

https://haystack.deepset.ai/cookbook/query_decomposition

https://python.langchain.com/docs/tutorials/rag/


Data Retreivers:

https://python.langchain.com/docs/tutorials/retrievers/

Self-Query:
https://python.langchain.com/docs/how_to/self_query/
https://github.com/langchain-ai/langchain/blob/master/cookbook/self_query_hotel_search.ipynb


-----------------------

# Import necessary modules
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.embeddings.sentence_transformer import SentenceTransformerEmbeddings
from langchain.vectorstores import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import create_retrieval_chain, create_history_aware_retriever
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_openai import ChatOpenAI

# Step 1: Define the contextualization prompt (for reformulating questions)
contextualize_q_system_prompt = """Given a chat history and the latest user question 
which might reference context in the chat history, formulate a standalone question 
which can be understood without the chat history. Do NOT answer the question, 
just reformulate it if needed and otherwise return it as is."""

contextualize_q_prompt = ChatPromptTemplate.from_messages([
    ("system", contextualize_q_system_prompt),
    MessagesPlaceholder("chat_history"),  # This allows access to chat history
    ("human", "{input}")  # Latest user input
])

# Step 2: Define the QA prompt template (for generating responses)
qa_system_prompt = """You are an assistant for question-answering tasks. 
Use the following pieces of retrieved context to answer the question. 
If you don't know the answer, just say that you don't know. 
Use three sentences maximum and keep the answer concise.\n{context}"""

qa_prompt = ChatPromptTemplate.from_messages([
    ("system", qa_system_prompt),
    MessagesPlaceholder("chat_history"),  # This allows access to chat history in responses
    ("human", "{input}")  # Latest user input
])

# Step 3: Load documents and prepare embeddings
documents = [
    {"page_content": "LangChain is a framework for building applications with LLMs.", "metadata": {}},
    {"page_content": "ConversationalRetrievalChain helps maintain context in chat-based applications.", "metadata": {}}
]

text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
texts = text_splitter.split_documents(documents)

embeddings = SentenceTransformerEmbeddings(model_name="all-MiniLM-L6-v2")
vectorDB = Chroma.from_documents(texts, embeddings)

# Step 4: Set up the language model and retriever
llm = ChatOpenAI(api_key="YOUR_OPENAI_API_KEY")
retriever = vectorDB.as_retriever()

# Step 5: Create history-aware retriever using the contextualization prompt
history_aware_retriever = create_history_aware_retriever(
    llm=llm,
    retriever=retriever,
    prompt=contextualize_q_prompt  # Use updated contextualization prompt here
)

# Step 6: Create the question-answering chain using the QA prompt template
question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)

# Step 7: Combine everything into a ConversationalRetrievalChain
retrieval_chain = create_retrieval_chain(
    retriever=history_aware_retriever,
    combine_docs_chain=question_answer_chain
)

# Step 8: Invoke the chain with user input and chat history
user_input = "Can you elaborate on LangChain?"
chat_history = []  # Initialize empty chat history

response = retrieval_chain.invoke({"input": user_input, "chat_history": chat_history})
print(response["answer"])

-------------------

     # Create the custom RAG prompt
        self.prompt = ChatPromptTemplate.from_messages([
            ("system", f"""{self._base_system_prompt}
            
            Use the following email content to answer the user's question.
            
            Rules:
            1. Answer ONLY the question asked - no additional context or explanations
            2. If you don't know the answer, just say "I don't know"
            3. Keep answers short and direct
            4. Do not include system messages, UI prompts, or follow-up questions
            
            Context from emails: {{context}}"""),
            MessagesPlaceholder(variable_name="chat_history"),
            ("human", "{{question}}")
        ])
        self.prompt2 = ChatPromptTemplate.from_messages([
            ("system", f"""{self._base_system_prompt}
            
            Use the following pieces of context to answer the question at the end.
            If you don't know the answer, just say that you don't know, don't try to make up an answer.
            Keep answers short and direct.
            
            Context from emails: {{context}}

            Question: {{question}}
            Answer:"""),
            MessagesPlaceholder(variable_name="chat_history")
        ])

-----------------------------

Llama 3 8B special tokens:
<|begin_of_text|>: Marks the start of the input.

<|start_header_id|>system<|end_header_id>: Denotes the beginning of a system message.
<|eot_id|>: Indicates the end of a turn or text (end of transmission).
<|end_of_text|>: Marks the end of the entire sequence.





<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are an
AI assistant that rephrases questions concisely without adding extra
information.<|eot_id|><|start_header_id|>user<|end_header_id|> Rephrase the
following question and return only a single concise response: What is Gopal
Srinivasan's email?<|eot_id|><|start_header_id|>assistant<|end_header_id>
<|end_of_text|>

-----------------------------

To Learn

ChatPromptTemplate
MessagesPlaceholder