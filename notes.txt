docker login nvcr.io --username '$oauthtoken' --password "${NGC_API_KEY}"

export LOCAL_NIM_CACHE=~/.cache/nim
mkdir -p "$LOCAL_NIM_CACHE"
chmod 777 "$LOCAL_NIM_CACHE"

docker run -d --name meta-llama3-8b-instruct --gpus all -e NGC_API_KEY -v "$LOCAL_NIM_CACHE:/opt/nim/.cache" -u $(id -u) -p 8000:8000 nvcr.io/nim/meta/llama3-8b-instruct:1.0.0

curl -X 'POST' \
   "http://0.0.0.0:8000/v1/completions" \
   -H "accept: application/json" \
   -H "Content-Type: application/json" \
   -d '{"model": "meta/llama3-8b-instruct", "prompt": "Once upon a time", "max_tokens": 64}'


Embedding Model: NV-Embed-QA, 

Reranking Models: 
- nv-rerankqa-mistral-4b-v3: https://build.nvidia.com/nvidia/nv-rerankqa-mistral-4b-v3/
- llama-3.2-nv-rerankqa-1b-v2: https://build.nvidia.com/nvidia/llama-3_2-nv-rerankqa-1b-v2


LLM: 
- meta-llama3-8b-instruct:  https://build.nvidia.com/meta/llama3-8b
- llama-3_1-8b-instruct: https://build.nvidia.com/meta/llama-3_1-8b-instruct

- deepseek-r1-distill-llama-8b: 
   - Model: https://build.nvidia.com/deepseek-ai/deepseek-r1-distill-llama-8b, 
   - NIM: https://catalog.ngc.nvidia.com/orgs/nim/teams/deepseek-ai/containers/deepseek-r1-distill-llama-8b

- mistral-nemo-12b-instruct: https://build.nvidia.com/nv-mistralai/mistral-nemo-12b-instruct

- mixtral-8x7b-instruct: https://build.nvidia.com/mistralai/mixtral-8x7b-instruct/modelcard

Other Models compatible with meta-llama3-8b-instruct
Multilingual-E5-Large: A general-purpose embedding model also used in RAG workflows 
Llama-Text-Embed-V2: Specifically designed to align with the LLaMA family of models, making it a natural fit for meta-llama3
Cohere Embed Models: Such as embed-english-v3 or embed-multilingual-v3, which provide high-quality embeddings for multilingual or English text

When selecting an embedding model:
- Ensure it supports your specific use case (e.g., query and passage embeddings).
- Check its compatibility with your deployment infrastructure (e.g., NVIDIA NIMs, Hugging Face, etc.).
- Test its alignment with meta-llama3-8b-instruct to ensure semantic coherence in retrieved results. 

Evaluating RAG flows: https://medium.com/@zilliz_learn/optimizing-rag-applications-a-guide-to-methodologies-metrics-and-evaluation-tools-for-enhanced-a9ae3d9c7149

python data_prep.py --vectordb-dir hrishim_vectordb --save-frequency 10 hrishi_gmail.mbox

python data_prep.py --vectordb-dir hrishim_vectordb --save-frequency 10 --resume hrishi_gmail.mbox